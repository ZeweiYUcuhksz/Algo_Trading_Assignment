{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import backtrader as bt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from copy import deepcopy\n",
    "from datas import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、数据准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 读取日度行情表\n",
    "\n",
    "表内字段就是 Backtrader 默认情况下要求输入的 7 个字段： 'datetime' 、'open'、'high'、'low'、'close'、'volume'、'openinterest'，外加一个 'sec_code' 股票代码字段。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "login success!\n",
      "login respond error_code:0\n",
      "login respond  error_msg:success\n",
      "query_hs300 error_code:0\n",
      "query_hs300  error_msg:success\n",
      "logout success!\n",
      "300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mj/q763xcj90k17wtcw8c2h9n_c0000gn/T/ipykernel_82864/2478071887.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  daily_price['datetime'] = pd.to_datetime(daily_price['datetime'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sec_code</th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>volume</th>\n",
       "      <th>openinterest</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-02</th>\n",
       "      <td>600000</td>\n",
       "      <td>12.61</td>\n",
       "      <td>12.72</td>\n",
       "      <td>12.77</td>\n",
       "      <td>12.60</td>\n",
       "      <td>313231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>600000</td>\n",
       "      <td>12.73</td>\n",
       "      <td>12.66</td>\n",
       "      <td>12.80</td>\n",
       "      <td>12.66</td>\n",
       "      <td>378391</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>600000</td>\n",
       "      <td>12.70</td>\n",
       "      <td>12.66</td>\n",
       "      <td>12.73</td>\n",
       "      <td>12.62</td>\n",
       "      <td>278838</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>600000</td>\n",
       "      <td>12.67</td>\n",
       "      <td>12.69</td>\n",
       "      <td>12.71</td>\n",
       "      <td>12.62</td>\n",
       "      <td>310267</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>600000</td>\n",
       "      <td>12.69</td>\n",
       "      <td>12.68</td>\n",
       "      <td>12.71</td>\n",
       "      <td>12.63</td>\n",
       "      <td>313899</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-27</th>\n",
       "      <td>300315</td>\n",
       "      <td>4.63</td>\n",
       "      <td>4.47</td>\n",
       "      <td>4.63</td>\n",
       "      <td>4.44</td>\n",
       "      <td>972028</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-28</th>\n",
       "      <td>300315</td>\n",
       "      <td>4.48</td>\n",
       "      <td>4.47</td>\n",
       "      <td>4.54</td>\n",
       "      <td>4.44</td>\n",
       "      <td>744388</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-29</th>\n",
       "      <td>300315</td>\n",
       "      <td>4.45</td>\n",
       "      <td>4.45</td>\n",
       "      <td>4.51</td>\n",
       "      <td>4.40</td>\n",
       "      <td>718864</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-30</th>\n",
       "      <td>300315</td>\n",
       "      <td>4.43</td>\n",
       "      <td>4.77</td>\n",
       "      <td>4.88</td>\n",
       "      <td>4.42</td>\n",
       "      <td>1878547</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-31</th>\n",
       "      <td>300315</td>\n",
       "      <td>4.76</td>\n",
       "      <td>4.86</td>\n",
       "      <td>4.95</td>\n",
       "      <td>4.72</td>\n",
       "      <td>1621918</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>286405 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           sec_code   open  close   high    low   volume  openinterest\n",
       "datetime                                                              \n",
       "2018-01-02   600000  12.61  12.72  12.77  12.60   313231             0\n",
       "2018-01-03   600000  12.73  12.66  12.80  12.66   378391             0\n",
       "2018-01-04   600000  12.70  12.66  12.73  12.62   278838             0\n",
       "2018-01-05   600000  12.67  12.69  12.71  12.62   310267             0\n",
       "2018-01-08   600000  12.69  12.68  12.71  12.63   313899             0\n",
       "...             ...    ...    ...    ...    ...      ...           ...\n",
       "2021-12-27   300315   4.63   4.47   4.63   4.44   972028             0\n",
       "2021-12-28   300315   4.48   4.47   4.54   4.44   744388             0\n",
       "2021-12-29   300315   4.45   4.45   4.51   4.40   718864             0\n",
       "2021-12-30   300315   4.43   4.77   4.88   4.42  1878547             0\n",
       "2021-12-31   300315   4.76   4.86   4.95   4.72  1621918             0\n",
       "\n",
       "[286405 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year = 2018\n",
    "end_year = 2021\n",
    "list_assets, df_assets = get_hs300_stocks(f'{year}-01-01')\n",
    "# df_org= get_all_date_data(f'{year}-01-01', f'{year+1}-01-01', list_assets)\n",
    "df_org= get_all_date_data(f'{year}-01-01', f'{end_year + 1}-01-01', list_assets)\n",
    "df1 = df_org.rename(columns={\n",
    "        \"date\": \"datetime\", \n",
    "        \"asset\": \"sec_code\"})\n",
    "df1[\"openinterest\"] = 0\n",
    "daily_price=df1[['sec_code','datetime', \"open\", \"close\", \"high\", \"low\", \"volume\", 'openinterest']]\n",
    "daily_price['datetime'] = pd.to_datetime(daily_price['datetime'])\n",
    "\n",
    "# 以 datetime 为 index，类型为 datetime 或 date 类型，Datafeeds 默认情况下是将 index 匹配给 datetime 字段；\n",
    "daily_price = daily_price.set_index(['datetime'])\n",
    "daily_price"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 生成调仓信息表\n",
    "\n",
    "表内数据说明：\n",
    "\n",
    "+ trade_date： 调仓期（每月最后一个交易日）;\n",
    "\n",
    "+ sec_code：持仓成分股；\n",
    "\n",
    "+ weight：持仓权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mj/q763xcj90k17wtcw8c2h9n_c0000gn/T/ipykernel_82864/3780514021.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_2['date'] = pd.to_datetime(df_2['date'])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot handle a non-unique multi-index!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/alanyu/Downloads/alphas-main/backtest_alpha019.ipynb 单元格 6\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alanyu/Downloads/alphas-main/backtest_alpha019.ipynb#W5sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m alpha \u001b[39m=\u001b[39m alpha\u001b[39m.\u001b[39mset_index([\u001b[39m'\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39masset\u001b[39m\u001b[39m'\u001b[39m], drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alanyu/Downloads/alphas-main/backtest_alpha019.ipynb#W5sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m alpha\u001b[39m.\u001b[39msort_index(inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/alanyu/Downloads/alphas-main/backtest_alpha019.ipynb#W5sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m ret \u001b[39m=\u001b[39m get_clean_factor_and_forward_returns(alpha, close,quantiles\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alanyu/Downloads/alphas-main/backtest_alpha019.ipynb#W5sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m ret \u001b[39m=\u001b[39m ret\u001b[39m.\u001b[39mreset_index()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/alanyu/Downloads/alphas-main/backtest_alpha019.ipynb#W5sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m ret \u001b[39m=\u001b[39m ret[ret[\u001b[39m'\u001b[39m\u001b[39mfactor_quantile\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m5\u001b[39m]\n",
      "File \u001b[0;32m~/Downloads/alphas-main/alphalens/utils.py:835\u001b[0m, in \u001b[0;36mget_clean_factor_and_forward_returns\u001b[0;34m(factor, prices, groupby, binning_by_group, quantiles, bins, periods, filter_zscore, groupby_labels, max_loss, zero_aware, cumulative_returns)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    679\u001b[0m \u001b[39mFormats the factor data, pricing data, and group mappings into a DataFrame\u001b[39;00m\n\u001b[1;32m    680\u001b[0m \u001b[39mthat contains aligned MultiIndex indices of timestamp and asset. The\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[39m    For use when forward returns are already available.\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    827\u001b[0m forward_returns \u001b[39m=\u001b[39m compute_forward_returns(\n\u001b[1;32m    828\u001b[0m     factor,\n\u001b[1;32m    829\u001b[0m     prices,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    832\u001b[0m     cumulative_returns,\n\u001b[1;32m    833\u001b[0m )\n\u001b[0;32m--> 835\u001b[0m factor_data \u001b[39m=\u001b[39m get_clean_factor(factor, forward_returns, groupby\u001b[39m=\u001b[39mgroupby,\n\u001b[1;32m    836\u001b[0m                                groupby_labels\u001b[39m=\u001b[39mgroupby_labels,\n\u001b[1;32m    837\u001b[0m                                quantiles\u001b[39m=\u001b[39mquantiles, bins\u001b[39m=\u001b[39mbins,\n\u001b[1;32m    838\u001b[0m                                binning_by_group\u001b[39m=\u001b[39mbinning_by_group,\n\u001b[1;32m    839\u001b[0m                                max_loss\u001b[39m=\u001b[39mmax_loss, zero_aware\u001b[39m=\u001b[39mzero_aware)\n\u001b[1;32m    841\u001b[0m \u001b[39mreturn\u001b[39;00m factor_data\n",
      "File \u001b[0;32m~/Downloads/alphas-main/alphalens/utils.py:641\u001b[0m, in \u001b[0;36mget_clean_factor\u001b[0;34m(factor, forward_returns, groupby, binning_by_group, quantiles, bins, groupby_labels, max_loss, zero_aware)\u001b[0m\n\u001b[1;32m    632\u001b[0m no_raise \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m \u001b[39mif\u001b[39;00m max_loss \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    633\u001b[0m quantile_data \u001b[39m=\u001b[39m quantize_factor(\n\u001b[1;32m    634\u001b[0m     merged_data,\n\u001b[1;32m    635\u001b[0m     quantiles,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    639\u001b[0m     zero_aware\n\u001b[1;32m    640\u001b[0m )\n\u001b[0;32m--> 641\u001b[0m merged_data[\u001b[39m'\u001b[39m\u001b[39mfactor_quantile\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m quantile_data\n\u001b[1;32m    643\u001b[0m merged_data \u001b[39m=\u001b[39m merged_data\u001b[39m.\u001b[39mdropna()\n\u001b[1;32m    645\u001b[0m binning_amount \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(\u001b[39mlen\u001b[39m(merged_data\u001b[39m.\u001b[39mindex))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:3950\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3947\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   3948\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3949\u001b[0m     \u001b[39m# set column\u001b[39;00m\n\u001b[0;32m-> 3950\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_item(key, value)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:4143\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4133\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_set_item\u001b[39m(\u001b[39mself\u001b[39m, key, value) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   4134\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4135\u001b[0m \u001b[39m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[1;32m   4136\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4141\u001b[0m \u001b[39m    ensure homogeneity.\u001b[39;00m\n\u001b[1;32m   4142\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4143\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sanitize_column(value)\n\u001b[1;32m   4145\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   4146\u001b[0m         key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\n\u001b[1;32m   4147\u001b[0m         \u001b[39mand\u001b[39;00m value\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   4148\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_extension_array_dtype(value)\n\u001b[1;32m   4149\u001b[0m     ):\n\u001b[1;32m   4150\u001b[0m         \u001b[39m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[1;32m   4151\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mis_unique \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:4867\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   4865\u001b[0m     \u001b[39mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\n\u001b[1;32m   4866\u001b[0m \u001b[39melif\u001b[39;00m is_dict_like(value):\n\u001b[0;32m-> 4867\u001b[0m     \u001b[39mreturn\u001b[39;00m _reindex_for_setitem(Series(value), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\n\u001b[1;32m   4869\u001b[0m \u001b[39mif\u001b[39;00m is_list_like(value):\n\u001b[1;32m   4870\u001b[0m     com\u001b[39m.\u001b[39mrequire_length_match(value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:11615\u001b[0m, in \u001b[0;36m_reindex_for_setitem\u001b[0;34m(value, index)\u001b[0m\n\u001b[1;32m  11611\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m  11612\u001b[0m     \u001b[39m# raised in MultiIndex.from_tuples, see test_insert_error_msmgs\u001b[39;00m\n\u001b[1;32m  11613\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m value\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mis_unique:\n\u001b[1;32m  11614\u001b[0m         \u001b[39m# duplicate axis\u001b[39;00m\n\u001b[0;32m> 11615\u001b[0m         \u001b[39mraise\u001b[39;00m err\n\u001b[1;32m  11617\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m  11618\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mincompatible index of inserted column with frame index\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m  11619\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m  11620\u001b[0m \u001b[39mreturn\u001b[39;00m reindexed_value\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:11610\u001b[0m, in \u001b[0;36m_reindex_for_setitem\u001b[0;34m(value, index)\u001b[0m\n\u001b[1;32m  11608\u001b[0m \u001b[39m# GH#4107\u001b[39;00m\n\u001b[1;32m  11609\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m> 11610\u001b[0m     reindexed_value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39mreindex(index)\u001b[39m.\u001b[39m_values\n\u001b[1;32m  11611\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m  11612\u001b[0m     \u001b[39m# raised in MultiIndex.from_tuples, see test_insert_error_msmgs\u001b[39;00m\n\u001b[1;32m  11613\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m value\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mis_unique:\n\u001b[1;32m  11614\u001b[0m         \u001b[39m# duplicate axis\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/series.py:4918\u001b[0m, in \u001b[0;36mSeries.reindex\u001b[0;34m(self, index, axis, method, copy, level, fill_value, limit, tolerance)\u001b[0m\n\u001b[1;32m   4901\u001b[0m \u001b[39m@doc\u001b[39m(\n\u001b[1;32m   4902\u001b[0m     NDFrame\u001b[39m.\u001b[39mreindex,  \u001b[39m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m   4903\u001b[0m     klass\u001b[39m=\u001b[39m_shared_doc_kwargs[\u001b[39m\"\u001b[39m\u001b[39mklass\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4916\u001b[0m     tolerance\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   4917\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Series:\n\u001b[0;32m-> 4918\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mreindex(\n\u001b[1;32m   4919\u001b[0m         index\u001b[39m=\u001b[39mindex,\n\u001b[1;32m   4920\u001b[0m         method\u001b[39m=\u001b[39mmethod,\n\u001b[1;32m   4921\u001b[0m         copy\u001b[39m=\u001b[39mcopy,\n\u001b[1;32m   4922\u001b[0m         level\u001b[39m=\u001b[39mlevel,\n\u001b[1;32m   4923\u001b[0m         fill_value\u001b[39m=\u001b[39mfill_value,\n\u001b[1;32m   4924\u001b[0m         limit\u001b[39m=\u001b[39mlimit,\n\u001b[1;32m   4925\u001b[0m         tolerance\u001b[39m=\u001b[39mtolerance,\n\u001b[1;32m   4926\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py:5360\u001b[0m, in \u001b[0;36mNDFrame.reindex\u001b[0;34m(self, labels, index, columns, axis, method, copy, level, fill_value, limit, tolerance)\u001b[0m\n\u001b[1;32m   5357\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_multi(axes, copy, fill_value)\n\u001b[1;32m   5359\u001b[0m \u001b[39m# perform the reindex on the axes\u001b[39;00m\n\u001b[0;32m-> 5360\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_axes(\n\u001b[1;32m   5361\u001b[0m     axes, level, limit, tolerance, method, fill_value, copy\n\u001b[1;32m   5362\u001b[0m )\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mreindex\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py:5375\u001b[0m, in \u001b[0;36mNDFrame._reindex_axes\u001b[0;34m(self, axes, level, limit, tolerance, method, fill_value, copy)\u001b[0m\n\u001b[1;32m   5372\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m   5374\u001b[0m ax \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_axis(a)\n\u001b[0;32m-> 5375\u001b[0m new_index, indexer \u001b[39m=\u001b[39m ax\u001b[39m.\u001b[39mreindex(\n\u001b[1;32m   5376\u001b[0m     labels, level\u001b[39m=\u001b[39mlevel, limit\u001b[39m=\u001b[39mlimit, tolerance\u001b[39m=\u001b[39mtolerance, method\u001b[39m=\u001b[39mmethod\n\u001b[1;32m   5377\u001b[0m )\n\u001b[1;32m   5379\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_axis_number(a)\n\u001b[1;32m   5380\u001b[0m obj \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39m_reindex_with_indexers(\n\u001b[1;32m   5381\u001b[0m     {axis: [new_index, indexer]},\n\u001b[1;32m   5382\u001b[0m     fill_value\u001b[39m=\u001b[39mfill_value,\n\u001b[1;32m   5383\u001b[0m     copy\u001b[39m=\u001b[39mcopy,\n\u001b[1;32m   5384\u001b[0m     allow_dups\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   5385\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:4272\u001b[0m, in \u001b[0;36mIndex.reindex\u001b[0;34m(self, target, method, level, limit, tolerance)\u001b[0m\n\u001b[1;32m   4268\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_indexer(\n\u001b[1;32m   4269\u001b[0m         target, method\u001b[39m=\u001b[39mmethod, limit\u001b[39m=\u001b[39mlimit, tolerance\u001b[39m=\u001b[39mtolerance\n\u001b[1;32m   4270\u001b[0m     )\n\u001b[1;32m   4271\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_multi:\n\u001b[0;32m-> 4272\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcannot handle a non-unique multi-index!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   4273\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_unique:\n\u001b[1;32m   4274\u001b[0m     \u001b[39m# GH#42568\u001b[39;00m\n\u001b[1;32m   4275\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcannot reindex on an axis with duplicate labels\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: cannot handle a non-unique multi-index!"
     ]
    }
   ],
   "source": [
    "from alphalens.utils import get_clean_factor_and_forward_returns\n",
    "from alphalens.tears import create_full_tear_sheet\n",
    "\n",
    "df_2 = df_org[['date', 'asset', \"close\"]]\n",
    "df_2['date'] = pd.to_datetime(df_2['date'])\n",
    "# print(df_all)\n",
    "\n",
    "close = df_2.pivot(index='date', columns='asset', values='close')\n",
    "\n",
    "alpha_num = 19\n",
    "alpha_name = 'Myalphas'\n",
    "\n",
    "# 读取已经计算好的因子\n",
    "# alpha = pd.read_csv('alphas/{}/{}/alpha{:03d}.csv'.format(alpha_name, year, alpha_num))\n",
    "alpha = pd.read_csv('alphas/{}/Total/alpha{:03d}.csv'.format(alpha_name, alpha_num))\n",
    "\n",
    "# 筛选出今年的数据，需与股票收盘日期区间一致\n",
    "alpha = alpha[(alpha['date'] >= f'{year}-01-01') & (alpha['date'] <= f'{end_year + 1}-01-01')]\n",
    "\n",
    "# 因子矩阵转换为一维数据(alphalens需要的格式)\n",
    "alpha = alpha.melt(id_vars=['date'], var_name='asset', value_name='factor' )\n",
    "\n",
    "# date列转为日期格式\n",
    "alpha['date'] = pd.to_datetime(alpha['date'])\n",
    "alpha = alpha[['date', 'asset', 'factor']]\n",
    "\n",
    "# 设置二级索引\n",
    "alpha = alpha.set_index(['date', 'asset'], drop=True)\n",
    "alpha.sort_index(inplace=True)\n",
    "\n",
    "\n",
    "ret = get_clean_factor_and_forward_returns(alpha, close,quantiles=5)\n",
    "ret = ret.reset_index()\n",
    "ret = ret[ret['factor_quantile'] == 5]\n",
    "# ret['week'] =  pd.to_datetime(ret['date']).dt.weekday\n",
    "# ret = ret[ret['week'] == 4]\n",
    "ret = ret[['date','asset']]\n",
    "ret['weight'] = 1/60\n",
    "trade_info = ret.rename(columns={\n",
    "        \"date\": \"trade_date\", \n",
    "        \"asset\": \"sec_code\"})\n",
    "trade_info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、 回测分析"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 加载策略和数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600000 Done !\n",
      "600008 Done !\n",
      "600009 Done !\n",
      "600010 Done !\n",
      "600011 Done !\n",
      "600015 Done !\n",
      "600016 Done !\n",
      "600018 Done !\n",
      "600019 Done !\n",
      "600021 Done !\n",
      "600023 Done !\n",
      "600028 Done !\n",
      "600029 Done !\n",
      "600030 Done !\n",
      "600031 Done !\n",
      "600036 Done !\n",
      "600038 Done !\n",
      "600048 Done !\n",
      "600050 Done !\n",
      "600061 Done !\n",
      "600066 Done !\n",
      "600068 Done !\n",
      "600074 Done !\n",
      "600085 Done !\n",
      "600089 Done !\n",
      "600100 Done !\n",
      "600104 Done !\n",
      "600109 Done !\n",
      "600111 Done !\n",
      "600115 Done !\n",
      "600118 Done !\n",
      "600153 Done !\n",
      "600157 Done !\n",
      "600170 Done !\n",
      "600177 Done !\n",
      "600188 Done !\n",
      "600196 Done !\n",
      "600208 Done !\n",
      "600219 Done !\n",
      "600221 Done !\n",
      "600233 Done !\n",
      "600271 Done !\n",
      "600276 Done !\n",
      "600297 Done !\n",
      "600309 Done !\n",
      "600332 Done !\n",
      "600340 Done !\n",
      "600352 Done !\n",
      "600362 Done !\n",
      "600369 Done !\n",
      "600372 Done !\n",
      "600373 Done !\n",
      "600376 Done !\n",
      "600383 Done !\n",
      "600390 Done !\n",
      "600406 Done !\n",
      "600415 Done !\n",
      "600436 Done !\n",
      "600482 Done !\n",
      "600485 Done !\n",
      "600489 Done !\n",
      "600498 Done !\n",
      "600518 Done !\n",
      "600519 Done !\n",
      "600522 Done !\n",
      "600535 Done !\n",
      "600547 Done !\n",
      "600549 Done !\n",
      "600570 Done !\n",
      "600583 Done !\n",
      "600585 Done !\n",
      "600588 Done !\n",
      "600606 Done !\n",
      "600637 Done !\n",
      "600649 Done !\n",
      "600660 Done !\n",
      "600663 Done !\n",
      "600674 Done !\n",
      "600682 Done !\n",
      "600685 Done !\n",
      "600688 Done !\n",
      "600690 Done !\n",
      "600703 Done !\n",
      "600704 Done !\n",
      "600705 Done !\n",
      "600739 Done !\n",
      "600741 Done !\n",
      "600795 Done !\n",
      "600804 Done !\n",
      "600816 Done !\n",
      "600820 Done !\n",
      "600827 Done !\n",
      "600837 Done !\n",
      "600871 Done !\n",
      "600886 Done !\n",
      "600887 Done !\n",
      "600893 Done !\n",
      "600895 Done !\n",
      "600900 Done !\n",
      "600909 Done !\n",
      "600919 Done !\n",
      "600926 Done !\n",
      "600958 Done !\n",
      "600959 Done !\n",
      "600977 Done !\n",
      "600999 Done !\n",
      "601006 Done !\n",
      "601009 Done !\n",
      "601012 Done !\n",
      "601018 Done !\n",
      "601021 Done !\n",
      "601088 Done !\n",
      "601099 Done !\n",
      "601111 Done !\n",
      "601117 Done !\n",
      "601118 Done !\n",
      "601155 Done !\n",
      "601163 Done !\n",
      "601166 Done !\n",
      "601169 Done !\n",
      "601186 Done !\n",
      "601198 Done !\n",
      "601211 Done !\n",
      "601212 Done !\n",
      "601216 Done !\n",
      "601225 Done !\n",
      "601228 Done !\n",
      "601229 Done !\n",
      "601288 Done !\n",
      "601318 Done !\n",
      "601328 Done !\n",
      "601333 Done !\n",
      "601336 Done !\n",
      "601375 Done !\n",
      "601377 Done !\n",
      "601390 Done !\n",
      "601398 Done !\n",
      "601555 Done !\n",
      "601600 Done !\n",
      "601601 Done !\n",
      "601607 Done !\n",
      "601608 Done !\n",
      "601611 Done !\n",
      "601618 Done !\n",
      "601628 Done !\n",
      "601633 Done !\n",
      "601668 Done !\n",
      "601669 Done !\n",
      "601688 Done !\n",
      "601718 Done !\n",
      "601727 Done !\n",
      "601766 Done !\n",
      "601788 Done !\n",
      "601800 Done !\n",
      "601818 Done !\n",
      "601857 Done !\n",
      "601866 Done !\n",
      "601872 Done !\n",
      "601877 Done !\n",
      "601878 Done !\n",
      "601881 Done !\n",
      "601888 Done !\n",
      "601898 Done !\n",
      "601899 Done !\n",
      "601901 Done !\n",
      "601919 Done !\n",
      "601933 Done !\n",
      "601939 Done !\n",
      "601958 Done !\n",
      "601966 Done !\n",
      "601985 Done !\n",
      "601988 Done !\n",
      "601989 Done !\n",
      "601991 Done !\n",
      "601992 Done !\n",
      "601997 Done !\n",
      "601998 Done !\n",
      "603160 Done !\n",
      "603799 Done !\n",
      "603833 Done !\n",
      "603858 Done !\n",
      "603993 Done !\n",
      "000001 Done !\n",
      "000002 Done !\n",
      "000008 Done !\n",
      "000060 Done !\n",
      "000063 Done !\n",
      "000069 Done !\n",
      "000100 Done !\n",
      "000157 Done !\n",
      "000166 Done !\n",
      "000333 Done !\n",
      "000338 Done !\n",
      "000402 Done !\n",
      "000413 Done !\n",
      "000415 Done !\n",
      "000423 Done !\n",
      "000425 Done !\n",
      "000503 Done !\n",
      "000538 Done !\n",
      "000540 Done !\n",
      "000559 Done !\n",
      "000568 Done !\n",
      "000623 Done !\n",
      "000625 Done !\n",
      "000627 Done !\n",
      "000630 Done !\n",
      "000651 Done !\n",
      "000671 Done !\n",
      "000686 Done !\n",
      "000709 Done !\n",
      "000723 Done !\n",
      "000725 Done !\n",
      "000728 Done !\n",
      "000738 Done !\n",
      "000750 Done !\n",
      "000768 Done !\n",
      "000776 Done !\n",
      "000783 Done !\n",
      "000792 Done !\n",
      "000826 Done !\n",
      "000839 Done !\n",
      "000858 Done !\n",
      "000876 Done !\n",
      "000895 Done !\n",
      "000898 Done !\n",
      "000938 Done !\n",
      "000959 Done !\n",
      "000961 Done !\n",
      "000963 Done !\n",
      "000983 Done !\n",
      "001979 Done !\n",
      "002007 Done !\n",
      "002008 Done !\n",
      "002024 Done !\n",
      "002027 Done !\n",
      "002044 Done !\n",
      "002065 Done !\n",
      "002074 Done !\n",
      "002081 Done !\n",
      "002142 Done !\n",
      "002146 Done !\n",
      "002153 Done !\n",
      "002174 Done !\n",
      "002202 Done !\n",
      "002230 Done !\n",
      "002236 Done !\n",
      "002241 Done !\n",
      "002252 Done !\n",
      "002292 Done !\n",
      "002294 Done !\n",
      "002304 Done !\n",
      "002310 Done !\n",
      "002352 Done !\n",
      "002385 Done !\n",
      "002411 Done !\n",
      "002415 Done !\n",
      "002424 Done !\n",
      "002426 Done !\n",
      "002450 Done !\n",
      "002456 Done !\n",
      "002460 Done !\n",
      "002465 Done !\n",
      "002466 Done !\n",
      "002468 Done !\n",
      "002470 Done !\n",
      "002475 Done !\n",
      "002500 Done !\n",
      "002508 Done !\n",
      "002555 Done !\n",
      "002558 Done !\n",
      "002572 Done !\n",
      "002594 Done !\n",
      "002601 Done !\n",
      "002602 Done !\n",
      "002608 Done !\n",
      "002624 Done !\n",
      "002673 Done !\n",
      "002714 Done !\n",
      "002736 Done !\n",
      "002739 Done !\n",
      "002797 Done !\n",
      "002831 Done !\n",
      "002839 Done !\n",
      "002841 Done !\n",
      "300003 Done !\n",
      "300015 Done !\n",
      "300017 Done !\n",
      "300024 Done !\n",
      "300027 Done !\n",
      "300033 Done !\n",
      "300059 Done !\n",
      "300070 Done !\n",
      "300072 Done !\n",
      "300122 Done !\n",
      "300124 Done !\n",
      "300136 Done !\n",
      "300144 Done !\n",
      "300251 Done !\n",
      "300315 Done !\n"
     ]
    }
   ],
   "source": [
    "# 回测策略\n",
    "class TestStrategy(bt.Strategy):\n",
    "    params = (\n",
    "        ('buy_stocks', None), # 传入各个调仓日的股票列表和相应的权重\n",
    "    )\n",
    "    def log(self, txt, dt=None):\n",
    "        ''' Logging function fot this strategy'''\n",
    "        dt = dt or self.datas[0].datetime.date(0)\n",
    "        print(f'{dt.isoformat()}, {txt}')\n",
    "\n",
    "    def __init__(self):\n",
    "         \n",
    "        self.trade_dates = pd.to_datetime(self.p.buy_stocks['trade_date'].unique()).tolist()\n",
    "        self.buy_stock = self.p.buy_stocks # 保留调仓信息\n",
    "        self.order_list = []  # 记录以往订单，在调仓日要全部取消未成交的订单\n",
    "        self.buy_stocks_pre = [] # 记录上一期持仓\n",
    "    \n",
    "    def next(self):\n",
    "        # 获取当前的回测时间点\n",
    "        dt = self.datas[0].datetime.date(0)\n",
    "        # 打印当前时刻的总资产\n",
    "        self.log('当前总资产 %.2f' %(self.broker.getvalue()))\n",
    "        \n",
    "        # 如果是调仓日，则进行调仓操作\n",
    "        if pd.to_datetime(dt) in self.trade_dates:\n",
    "            print(f\"--------------{dt} 为调仓日----------\")\n",
    "            #取消之前所下的没成交也未到期的订单\n",
    "            if len(self.order_list) > 0:\n",
    "                print(\"--------------- 撤销未完成的订单 -----------------\")\n",
    "                for od in self.order_list:\n",
    "                    # 如果订单未完成，则撤销订单\n",
    "                    self.cancel(od) \n",
    "                 #重置订单列表\n",
    "                self.order_list = [] \n",
    "\n",
    "            # 提取当前调仓日的持仓列表\n",
    "            buy_stocks_data = self.buy_stock.query(f\"trade_date=='{dt}'\")\n",
    "            long_list = buy_stocks_data['sec_code'].tolist()\n",
    "            print('long_list', long_list)  # 打印持仓列表\n",
    "\n",
    "            # 对现有持仓中，调仓后不再继续持有的股票进行卖出平仓\n",
    "            sell_stock = [i for i in self.buy_stocks_pre if i not in long_list]\n",
    "            print('sell_stock', sell_stock)\n",
    "            if sell_stock:\n",
    "                print(\"-----------对不再持有的股票进行平仓--------------\")\n",
    "                for stock in sell_stock:\n",
    "                    data = self.getdatabyname(stock)\n",
    "                    if self.getposition(data).size > 0 :\n",
    "                        od = self.close(data=data)  \n",
    "                        self.order_list.append(od) # 记录卖出订单\n",
    "\n",
    "            # 买入此次调仓的股票：多退少补原则\n",
    "            print(\"-----------买入此次调仓期的股票--------------\")\n",
    "            for stock in long_list:\n",
    "                w = buy_stocks_data.query(f\"sec_code=='{stock}'\")['weight'].iloc[0] # 提取持仓权重\n",
    "                data = self.getdatabyname(stock)\n",
    "                order = self.order_target_percent(data=data, target=w*0.95) # 为减少可用资金不足的情况，留 5% 的现金做备用\n",
    "                self.order_list.append(order)\n",
    "\n",
    "            self.buy_stocks_pre = long_list  # 保存此次调仓的股票列表\n",
    "        \n",
    "    #订单日志    \n",
    "    def notify_order(self, order):\n",
    "        # 未被处理的订单\n",
    "        if order.status in [order.Submitted, order.Accepted]:\n",
    "            return\n",
    "        # 已被处理的订单\n",
    "        if order.status in [order.Completed, order.Canceled, order.Margin]:\n",
    "            if order.isbuy():\n",
    "                self.log(\n",
    "                    'BUY EXECUTED, ref:%.0f, Price: %.2f, Cost: %.2f, Comm %.2f, Size: %.2f, Stock: %s' %\n",
    "                    (order.ref,\n",
    "                     order.executed.price,\n",
    "                     order.executed.value,\n",
    "                     order.executed.comm,\n",
    "                     order.executed.size,\n",
    "                     order.data._name))\n",
    "            else:  # Sell\n",
    "                self.log('SELL EXECUTED, ref:%.0f, Price: %.2f, Cost: %.2f, Comm %.2f, Size: %.2f, Stock: %s' %\n",
    "                        (order.ref,\n",
    "                         order.executed.price,\n",
    "                         order.executed.value,\n",
    "                         order.executed.comm,\n",
    "                         order.executed.size,\n",
    "                         order.data._name))\n",
    "\n",
    "# 实例化大脑\n",
    "cerebro_ = bt.Cerebro() \n",
    "\n",
    "# 按股票代码，依次循环传入数据\n",
    "for stock in daily_price['sec_code'].unique():\n",
    "    # 日期对齐\n",
    "    data = pd.DataFrame(index=daily_price.index.unique())\n",
    "    df = daily_price.query(f\"sec_code=='{stock}'\")[['open','high','low','close','volume','openinterest']]\n",
    "    data_ = pd.merge(data, df, left_index=True, right_index=True, how='left')\n",
    "    data_.loc[:,['volume','openinterest']] = data_.loc[:,['volume','openinterest']].fillna(0)\n",
    "    data_.loc[:,['open','high','low','close']] = data_.loc[:,['open','high','low','close']].fillna(method='pad')\n",
    "    # data_.loc[:,['open','high','low','close']] = data_.loc[:,['open','high','low','close']].fillna(0)\n",
    "    datafeed = bt.feeds.PandasData(dataname=data_, fromdate=datetime.datetime(year,1,1), todate=datetime.datetime(end_year + 1,1,1))\n",
    "    cerebro_.adddata(datafeed, name=stock)\n",
    "    print(f\"{stock} Done !\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 添加分析器并执行回测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cerebro = deepcopy(cerebro_)  # 深度复制已经导入数据的 cerebro_，避免重复导入数据 \n",
    "# 初始资金 100,000,000    \n",
    "cerebro.broker.setcash(100000.0) \n",
    "# cerebro.broker.setcommission(commission=0.0015)\n",
    "# 添加策略\n",
    "cerebro.addstrategy(TestStrategy, buy_stocks=trade_info) # 通过修改参数 buy_stocks ，使用同一策略回测不同的持仓列表\n",
    "\n",
    "# 添加分析指标\n",
    "# 返回年初至年末的年度收益率\n",
    "cerebro.addanalyzer(bt.analyzers.AnnualReturn, _name='_AnnualReturn')\n",
    "# 计算最大回撤相关指标\n",
    "cerebro.addanalyzer(bt.analyzers.DrawDown, _name='_DrawDown')\n",
    "# 计算年化收益\n",
    "cerebro.addanalyzer(bt.analyzers.Returns, _name='_Returns', tann=252)\n",
    "# 计算年化夏普比率\n",
    "cerebro.addanalyzer(bt.analyzers.SharpeRatio, _name='_SharpeRatio', timeframe=bt.TimeFrame.Days, annualize=True, riskfreerate=0) # 计算夏普比率\n",
    "cerebro.addanalyzer(bt.analyzers.SharpeRatio_A, _name='_SharpeRatio_A')\n",
    "# 返回收益率时序\n",
    "cerebro.addanalyzer(bt.analyzers.TimeReturn, _name='_TimeReturn')\n",
    "\n",
    "# 启动回测\n",
    "result = cerebro.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 回测结果分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- AnnualReturn -----------------\n",
      "OrderedDict([(2018, 34.28250910000015), (2019, -1.4889438432824031), (2020, 0.1487993715624727)])\n",
      "--------------- DrawDown -----------------\n",
      "AutoOrderedDict([('len', 486), ('drawdown', 149.8192332454337), ('moneydown', 5959820.290000041), ('max', AutoOrderedDict([('len', 486), ('drawdown', 198.95385616257084), ('moneydown', 7914399.260000076)]))])\n",
      "--------------- Returns -----------------\n",
      "OrderedDict([('rtot', -inf), ('ravg', -inf), ('rnorm', -inf), ('rnorm100', -inf)])\n",
      "--------------- SharpeRatio -----------------\n",
      "OrderedDict([('sharperatio', 0.32645861297702833)])\n",
      "--------------- SharpeRatio_A -----------------\n",
      "OrderedDict([('sharperatio', 0.6652847481750662)])\n"
     ]
    }
   ],
   "source": [
    "strat = result[0]\n",
    "print(\"--------------- AnnualReturn -----------------\")\n",
    "print(strat.analyzers._AnnualReturn.get_analysis())\n",
    "print(\"--------------- DrawDown -----------------\")\n",
    "print(strat.analyzers._DrawDown.get_analysis())\n",
    "print(\"--------------- Returns -----------------\")\n",
    "print(strat.analyzers._Returns.get_analysis())\n",
    "print(\"--------------- SharpeRatio -----------------\")\n",
    "print(strat.analyzers._SharpeRatio.get_analysis())\n",
    "print(\"--------------- SharpeRatio_A -----------------\")\n",
    "print(strat.analyzers._SharpeRatio_A.get_analysis())\n",
    "\n",
    "# 绘制累计收益率\n",
    "# ret = pd.Series(strat.analyzers._TimeReturn.get_analysis())\n",
    "# (ret + 1).cumprod().plot(figsize=(12,6))\n",
    "\n",
    "# ret = [{\n",
    "#         '年度': year,\n",
    "#         '收益率': strat.analyzers._Returns.get_analysis()['rtot'],\n",
    "#         '日均收益率': strat.analyzers._Returns.get_analysis()['ravg'],\n",
    "#         '年化收益率': strat.analyzers._Returns.get_analysis()['rnorm'],\n",
    "#         '最大回撤(%)': strat.analyzers._DrawDown.get_analysis()['max']['drawdown']\n",
    "#         * (-1),\n",
    "#         '夏普比率': strat.analyzers._SharpeRatio.get_analysis()['sharperatio'],\n",
    "#     }]\n",
    "# pd.DataFrame(ret).sort_values(by='年化收益率', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "e42634819b8c191a5d07eaf23810ff32516dd8d3875f28ec3e488928fbd3c187"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
